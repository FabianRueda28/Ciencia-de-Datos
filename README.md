# Ciencia de Datos
<img src = 'https://github.com/FabianRueda28/Ciencia-de-Datos/blob/main/src/que-es-la-ciencia-de-datos.png' height = 300 >
¿Qué es la ciencia de datos?
La ciencia de datos es el estudio de datos con el fin de extraer información significativa para empresas. Es un enfoque multidisciplinario que combina principios y prácticas del campo de las matemáticas, la estadística, la inteligencia artificial y la ingeniería de computación para analizar grandes cantidades de datos. Este análisis permite que los científicos de datos planteen y respondan a preguntas como “qué pasó”, “por qué pasó”, “qué pasará” y “qué se puede hacer con los resultados”.

¿Que tecnologías se utilizan para la ciencia de datos? 

- Python = Lenguaje de alto nivel en el cual realizamos nuestros procesos de ETL, machine learning, deep learning, analisis de los datos entre otras funciones.

- SQL = Importante para poder guardar nuestros datos de forma ordenada en bases de datos relacionales teniendo en cuenta la magnitud del proyecto y cual es su escalabilidad. Nosotros vamos a utilizar MySQL.

- NoSQL = A difrencia de las bases de datos relacionales las bases de datos no relacionales permiten mayor mayor facilidad a la hora de escalar el proyecto, esto lo hacemos de forma vertical, incrementando los nodos y así su capacidad de almacenar datos, utiliza archivos tipo JSON y son de facil lectura, incluso se pueden hacer lecturas SQL dentro de estas mismas para realizar busquedas y actualizaciones de los datos.
En este caso vamos a utilizar MongoDB.

- Docker = Docker es un sistema operativo (o runtime) para contenedores. El motor de Docker se instala en cada servidor en el que desee ejecutar contenedores y proporciona un conjunto sencillo de comandos que puede utilizar para crear, iniciar o detener contenedores. Normalmento los utilizamos para proyectos en los cuales no tengamos las mismas versiones de sotfwares que los demas desarrolladores, pero tiene muchisimas más funciones que simplemente esa. 

- Hadoop = Hadoop es una estructura de software de código abierto para almacenar datos y ejecutar aplicaciones en clústeres de hardware comercial. Proporciona almacenamiento masivo para cualquier tipo de datos, enorme poder de procesamiento y la capacidad de procesar tareas o trabajos concurrentes virtualmente ilimitados. 

- Apache Spark = Apache Spark es un motor unificado de analíticas para procesar datos a gran escala que integra módulos para SQL, streaming, aprendizaje automático y procesamiento de grafos. Spark se puede ejecutar de forma independiente o en Apache Hadoop, Apache Mesos, Kubernetes, la nube y distintas fuentes de datos.

- ETL = Extracción, transformación y carga (ETL) es el proceso consistente en combinar datos de diferentes orígenes un gran repositorio central llamado almacenamiento de datos.

- Power BI = Es una solución de análisis empresarial basado en la nube, que permite unir diferentes fuentes de datos, analizarlos y presentar un análisis de estos a través de informes y paneles. Con Power BI se tiene de manera fácil acceso a datos dentro y fuera de la organización casi en cualquier dispositivo.

El Machine Learning = Es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos masivos y elaborar predicciones (análisis predictivo).

Ejemplos de machine learning o aprendizaje automático
Para que la plataforma tome una decisión sobre qué nuevas canciones o artistas recomendar a un oyente, los algoritmos de aprendizaje automático asocian las preferencias del oyente con otros oyentes que tienen un gusto musical similar.

